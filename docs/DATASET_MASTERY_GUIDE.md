# Dataset Mastery Guide: The GFashion Example

## 1. Introduction: Why Datasets Matter

In modern software architecture, a **Dataset** is not just a collection of tables; it is the **ground truth** of your domain. It represents the state of your world.

For **GRID**, datasets serve two critical roles:
1.  **Operational Memory**: Storing the immediate state of transactions (e.g., "Alice bought a Hoodie").
2.  **Analytical Foundation**: Providing the raw material for insights (e.g., "Hoodies are our top seller").

This guide demonstrates a complete, compliant workflow for managing datasets using GRID's architecture, specifically leveraging **Databricks** for scale and **SQLAlchemy** for code-first modeling.

## 2. The GFashion Data Model

We use a fictional e-commerce brand, **GFashion**, to demonstrate these concepts.

### Schema Overview

| Model | Description | Key Fields |
|-------|-------------|------------|
| **Product** | Items for sale | `product_id`, `name`, `price`, `category` |
| **Customer** | Registered users | `customer_id`, `email`, `country` |
| **Order** | Transaction headers | `order_id`, `total_amount`, `status` |
| **OrderItem** | Transaction details | `product_id`, `quantity`, `unit_price` |

### Code Pattern: The ORM Layer

We define these models in Python, not SQL. This is "Code-First" development.

```python
# vinci_code/database/models_gfashion.py

class Order(Base):
    __tablename__ = "orders"

    order_id: Mapped[str] = mapped_column(String, primary_key=True)
    total_amount: Mapped[Decimal] = mapped_column(Numeric(12, 2))

    # Relationships define the "Graph" of your data
    items: Mapped[List["OrderItem"]] = relationship(back_populates="order")
```

## 3. The Lifecycle Workflow

The demonstration script (`demos/dataset_lifecycle.py`) executes the 4 key stages of data life:

### Stage 1: Ingestion (Creation)
We seed the universe with reference data.
```python
p1 = Product(name="Neural Network Hoodie", price=Decimal("59.99"))
session.add(p1)
session.commit()
```
*Insight*: Use `session.merge()` for idempotent seeding to avoid duplicates.

### Stage 2: Operation (Transaction)
Business logic happens here. We create an order and its items in a single **atomic transaction**.
```python
new_order = Order(customer_id="C001", total_amount=total)
session.add(new_order)
session.commit()
```
*Insight*: Atomicity ensures that we never have an Order without Items. It's all or nothing.

### Stage 3: Integration (The GRID Link)
This is unique to GRID. We link external business data (GFashion) to internal system events.
```python
event = Event(
    event_type="ORDER_PLACED",
    description="Customer C001 placed order..."
)
```
*Insight*: This allows the AI system to "sense" the business world without owning the business data.

### Stage 4: Analysis (Insight)
We ask questions of the data.
```python
# "What is our revenue by category?"
stmt = select(Product.category, func.sum(...)).group_by(Product.category)
```
*Insight*: SQL is generated dynamically. This query runs on Databricks' powerful engine, not in Python memory.

## 4. Runtime Behavior & Verification

### Running the Demo
```bash
python demos/dataset_lifecycle.py
```

### Observed Output
```text
ðŸš€ Starting Dataset Lifecycle Demo

[1] INGESTION: Seeding Reference Data...
    âœ“ Seeded 3 products and 2 customers

[2] OPERATION: Simulating Order Transaction...
    âœ“ Order ORD-a1b2c3d4 created for alice@example.com
    âœ“ Total Amount: $90.99

[3] INTEGRATION: Logging GRID Event...
    âœ“ Event logged: ORDER_PLACED (ID: 123)

[4] ANALYSIS: Deriving Insights...
    ðŸ“Š Revenue by Category:
       - Footwear: $120.00
       - Apparel: $59.99
       - Accessories: $31.00

âœ¨ Demo Complete
```

### Key Insights
1.  **Latency**: The first connection to Databricks may take 1-2 seconds (handshake). Subsequent queries are sub-second.
2.  **Consistency**: The `Event` log provides a perfect audit trail of the `Order` creation.
3.  **Scalability**: This same pattern works for 10 orders or 10 million orders because the heavy lifting is pushed to the database engine.

## 5. Conclusion

By following this pattern, you ensure your software is:
- **Reliable**: ACID transactions protect data integrity.
- **Observable**: GRID Events track every major state change.
- **Scalable**: Databricks handles the compute, Python handles the logic.
