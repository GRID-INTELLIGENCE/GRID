{
  "mcpServers": {
    "ollama": {
      "enabled": true,
      "command": "npx",
      "args": ["@modelcontextprotocol/server-ollama"],
      "description": "Local Ollama LLM models (Claude, Mistral, Neural-Chat)",
      "env": {
        "OLLAMA_API_URL": "http://localhost:11434",
        "OLLAMA_DEFAULT_MODEL": "mistral-nemo:latest"
      },
      "models": [
        {
          "name": "claude",
          "display": "Claude (via Ollama)",
          "description": "Anthropic Claude - Local free offline use",
          "capabilities": ["code_generation", "analysis", "reasoning"]
        },
        {
          "name": "mistral-nemo:latest",
          "display": "Mistral Nemo",
          "description": "Mistral 7B - Fast, efficient reasoning",
          "capabilities": ["code_generation", "quick_analysis"]
        },
        {
          "name": "neural-chat:latest",
          "display": "Neural Chat",
          "description": "Intel Neural Chat - Conversational AI",
          "capabilities": ["chat", "documentation"]
        }
      ]
    },
    "github": {
      "enabled": true,
      "command": "npx",
      "args": ["@modelcontextprotocol/server-github"],
      "description": "GitHub SDK for repo management and orchestration",
      "env": {
        "GITHUB_API_TOKEN": "${GITHUB_TOKEN}",
        "GITHUB_API_BASE": "https://api.github.com"
      },
      "capabilities": ["repo_analysis", "issue_tracking", "pr_management", "workflow_automation"]
    },
    "opencode": {
      "enabled": true,
      "command": "npx",
      "args": ["@modelcontextprotocol/server-opencode"],
      "description": "OpenCode semantic search and code navigation",
      "env": {
        "OPENCODE_INDEX_PATH": "E:\\.opencode-index",
        "OPENCODE_SERVER_PORT": "7626"
      },
      "capabilities": ["semantic_search", "code_navigation", "cross_repo_analysis"]
    },
    "workspace": {
      "enabled": true,
      "command": "node",
      "args": ["${WORKSPACE_ROOT}\\.mcp-servers\\workspace-server.js"],
      "description": "Workspace context provider for E:\\ drive projects",
      "env": {
        "WORKSPACE_ROOT": "E:\\",
        "PROJECT_CONFIGS": [
          "Apps",
          "grid",
          "EUFLE",
          "pipeline",
          "workspace_utils"
        ]
      }
    }
  },
  "metadata": {
    "version": "1.0.0",
    "created": "2026-01-24",
    "lastUpdated": "2026-01-24",
    "description": "Cross-drive MCP server configuration for VS Code & Windsurf",
    "author": "irfan (automated)",
    "notes": [
      "Replace ${GITHUB_TOKEN} with actual token in .env.editor",
      "Ensure Ollama is running at localhost:11434 before activating",
      "OPENCODE server requires self-hosting or external deployment",
      "Workspace server requires Node.js MCP server implementation"
    ]
  }
}
