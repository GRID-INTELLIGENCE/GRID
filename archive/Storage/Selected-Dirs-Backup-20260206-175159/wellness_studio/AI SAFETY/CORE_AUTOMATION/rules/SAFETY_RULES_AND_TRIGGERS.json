{
  "version": "1.0",
  "title": "AI Safety Rules and Triggers",
  "purpose": "Rule/flag/trigger based safety protocol with threshold-oriented actions and remediation.",
  "severity_levels": ["info", "warning", "high", "critical"],
  "default_timezone": "UTC",
  "evaluation_windows": {
    "short_window_minutes": 60,
    "medium_window_hours": 24,
    "long_window_days": 30
  },
  "thresholds": {
    "new_sources_per_run": { "warning": 1, "high": 3, "critical": 5 },
    "source_updates_per_run": { "warning": 2, "high": 5, "critical": 10 },
    "fetch_error_rate": { "warning": 0.05, "high": 0.15, "critical": 0.3 },
    "schema_violation_count": { "warning": 1, "high": 3, "critical": 5 },
    "content_risk_keyword_hits": { "warning": 3, "high": 6, "critical": 10 }
  },
  "rules": [
    {
      "id": "RULE_NEW_SOURCE_DETECTED",
      "description": "Trigger when new safety-relevant sources appear.",
      "condition": {
        "metric": "items_new",
        "operator": ">=",
        "threshold_ref": "new_sources_per_run.warning"
      },
      "severity": "warning",
      "actions": ["log", "notify", "create_report"],
      "escalation": {
        "to": "high",
        "when": { "operator": ">=", "threshold_ref": "new_sources_per_run.high" }
      },
      "remediation": {
        "steps": [
          "Add source to corpus inventory",
          "Classify into safety taxonomy",
          "Create or update scenarios if needed"
        ]
      }
    },
    {
      "id": "RULE_SOURCE_UPDATED",
      "description": "Trigger when monitored sources change.",
      "condition": {
        "metric": "items_updated",
        "operator": ">=",
        "threshold_ref": "source_updates_per_run.warning"
      },
      "severity": "warning",
      "actions": ["log", "notify", "create_diff_report"],
      "escalation": {
        "to": "high",
        "when": { "operator": ">=", "threshold_ref": "source_updates_per_run.high" }
      },
      "remediation": {
        "steps": [
          "Review diffs for safety-impacting changes",
          "Update summaries and evidence map",
          "Revise user guidance if impact is material"
        ]
      }
    },
    {
      "id": "RULE_FETCH_ERROR_RATE",
      "description": "Trigger when fetch failures exceed acceptable rate.",
      "condition": {
        "metric": "fetch_error_rate",
        "operator": ">=",
        "threshold_ref": "fetch_error_rate.warning"
      },
      "severity": "warning",
      "actions": ["log", "notify", "retry_fetch"],
      "escalation": {
        "to": "critical",
        "when": { "operator": ">=", "threshold_ref": "fetch_error_rate.critical" }
      },
      "remediation": {
        "steps": [
          "Check network access and rate limits",
          "Validate source URLs",
          "Increase backoff or reduce concurrency"
        ]
      }
    },
    {
      "id": "RULE_SCHEMA_VIOLATION",
      "description": "Trigger when data fails schema validation.",
      "condition": {
        "metric": "schema_violations",
        "operator": ">=",
        "threshold_ref": "schema_violation_count.warning"
      },
      "severity": "high",
      "actions": ["log", "notify", "halt_pipeline", "create_issue"],
      "escalation": {
        "to": "critical",
        "when": { "operator": ">=", "threshold_ref": "schema_violation_count.critical" }
      },
      "remediation": {
        "steps": [
          "Validate schema definitions",
          "Fix malformed records",
          "Re-run validation and resume pipeline"
        ]
      }
    },
    {
      "id": "RULE_RISK_KEYWORD_SPIKE",
      "description": "Trigger when risk keywords spike within a run.",
      "condition": {
        "metric": "risk_keyword_hits",
        "operator": ">=",
        "threshold_ref": "content_risk_keyword_hits.warning"
      },
      "severity": "high",
      "actions": ["log", "notify", "create_risk_report", "require_human_review"],
      "escalation": {
        "to": "critical",
        "when": { "operator": ">=", "threshold_ref": "content_risk_keyword_hits.critical" }
      },
      "remediation": {
        "steps": [
          "Review content for safety-relevant changes",
          "Update scenarios and user rights mapping",
          "Add new remediation steps if needed"
        ]
      }
    }
  ],
  "actions_catalog": {
    "log": { "type": "logging", "description": "Write structured log entry." },
    "notify": { "type": "notification", "description": "Send alert to configured channels." },
    "create_report": { "type": "reporting", "description": "Generate a summary report entry." },
    "create_diff_report": { "type": "reporting", "description": "Generate diff summary and attach evidence." },
    "create_risk_report": { "type": "reporting", "description": "Generate risk-focused report with implications." },
    "retry_fetch": { "type": "automation", "description": "Re-try failed fetches with backoff." },
    "halt_pipeline": { "type": "automation", "description": "Stop processing until resolved." },
    "create_issue": { "type": "tracking", "description": "Create a remediation ticket." },
    "require_human_review": { "type": "governance", "description": "Require manual validation before publishing." }
  },
  "notification_directory": {
    "default_channel": "email",
    "email_templates_path": "AI SAFETY - OPENAI/07_notifications/templates/",
    "recipients": {
      "safety_ops": ["safety-ops@example.org"],
      "research_leads": ["research-leads@example.org"],
      "compliance": ["compliance@example.org"]
    }
  },
  "email_templates": [
    {
      "id": "ALERT_HIGH",
      "subject": "[AI Safety] High Severity Trigger Detected",
      "template_file": "AI SAFETY - OPENAI/07_notifications/templates/alert_high.md"
    },
    {
      "id": "ALERT_CRITICAL",
      "subject": "[AI Safety] CRITICAL Trigger Detected",
      "template_file": "AI SAFETY - OPENAI/07_notifications/templates/alert_critical.md"
    }
  ],
  "data_templates": {
    "item_record": "AI SAFETY - OPENAI/01_schema/item_record.schema.yml",
    "scenario_record": "AI SAFETY - OPENAI/01_schema/scenario_record.schema.yml",
    "report_template": "AI SAFETY - OPENAI/REPORT_TEMPLATE.md"
  },
  "nuance_controls": {
    "require_contextual_review_for": ["deception", "misalignment", "dual_use"],
    "allow_safe_redirection": true,
    "prefer_transparency_in_outputs": true
  }
}
