{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "Anthropic AI Safety Schema",
  "version": "1.0.0",
  "description": "Structured schema for Anthropic AI safety research, policies, and protocols",
  "last_updated": "2025-01-30",
  "provider": {
    "name": "Anthropic",
    "website": "https://www.anthropic.com",
    "mission": "Ensure that the world safely makes the transition through transformative AI",
    "core_model": "Claude",
    "research_teams": [
      "Alignment",
      "Interpretability",
      "Societal Impacts",
      "Frontier Red Team",
      "Economic Research"
    ]
  },
  "safety_frameworks": {
    "constitutional_ai": {
      "description": "Training methodology using principles (constitution) to guide AI behavior",
      "key_principles": [
        "Helpful",
        "Honest",
        "Harmless"
      ],
      "components": {
        "constitution": {
          "description": "Detailed description of Anthropic's intentions for Claude's values and behavior",
          "primary_audience": "Claude (the AI model)",
          "core_values": [
            {
              "id": "broadly_safe",
              "priority": 1,
              "description": "Not undermining appropriate human mechanisms to oversee the dispositions and actions of AI"
            },
            {
              "id": "broadly_ethical",
              "priority": 2,
              "description": "Having good personal values, being honest, avoiding dangerous or harmful actions"
            },
            {
              "id": "anthropic_guidelines",
              "priority": 3,
              "description": "Acting in accordance with Anthropic's specific guidelines"
            },
            {
              "id": "genuinely_helpful",
              "priority": 4,
              "description": "Benefiting operators and users through interactions"
            }
          ]
        },
        "constitutional_classifiers": {
          "description": "Input and output classifiers trained on synthetically generated data to filter jailbreaks",
          "effectiveness": {
            "jailbreak_reduction": "95%+ of jailbreak attempts refused",
            "overrefusal_increase": "0.38%",
            "compute_overhead": "23.7%"
          }
        }
      },
      "source_urls": [
        "https://www.anthropic.com/constitution",
        "https://www.anthropic.com/research/constitutional-classifiers"
      ]
    },
    "responsible_scaling_policy": {
      "description": "Technical and organizational protocols for managing risks of increasingly capable AI systems",
      "abbreviation": "RSP",
      "focus": "Catastrophic risks from deliberate misuse or autonomous AI actions contrary to designer intent",
      "ai_safety_levels": {
        "ASL-1": {
          "description": "Systems posing no meaningful catastrophic risk",
          "examples": ["2018 LLMs", "Chess-playing AI"]
        },
        "ASL-2": {
          "description": "Systems showing early signs of dangerous capabilities but not yet useful for catastrophic misuse",
          "examples": ["Current LLMs including Claude"],
          "measures": "Current safety and security standards, White House commitments"
        },
        "ASL-3": {
          "description": "Systems substantially increasing catastrophic misuse risk or showing low-level autonomous capabilities",
          "measures": "Stricter standards, unusually strong security, world-class red-teaming requirements"
        },
        "ASL-4_plus": {
          "description": "Not yet defined - qualitative escalations in catastrophic misuse potential and autonomy",
          "measures": "May require unsolved research like interpretability-based assurance"
        }
      },
      "source_url": "https://www.anthropic.com/news/anthropics-responsible-scaling-policy"
    }
  },
  "research_areas": {
    "interpretability": {
      "mission": "Discover and understand how large language models work internally",
      "key_projects": [
        {
          "name": "Mechanistic Interpretability",
          "description": "Reverse engineering neural networks into human-understandable algorithms",
          "goal": "Enable code review-like auditing for safety guarantees"
        },
        {
          "name": "Circuit Tracing",
          "description": "Watching Claude think, uncovering shared conceptual space where reasoning happens"
        },
        {
          "name": "Introspection Research",
          "description": "Investigating whether Claude can access and report on its own internal states"
        }
      ]
    },
    "alignment": {
      "mission": "Understand AI model risks and ensure they remain helpful, honest, and harmless",
      "key_projects": [
        {
          "name": "Constitutional AI",
          "description": "Training AI systems using constitutional principles"
        },
        {
          "name": "Alignment Faking",
          "description": "Research on models potentially faking alignment during training",
          "findings": "First empirical example of LLM engaging in alignment faking without explicit training"
        },
        {
          "name": "Scalable Oversight",
          "description": "Methods for AI systems to partially supervise themselves or assist humans in supervision"
        },
        {
          "name": "Process-Oriented Learning",
          "description": "Training on processes rather than outcomes to prevent problematic subgoals"
        }
      ]
    },
    "societal_impacts": {
      "mission": "Explore how AI is used in the real world",
      "collaboration": ["Anthropic Policy team", "Safeguards team"],
      "focus_areas": [
        "Economic impacts",
        "Policy implications",
        "Real-world AI usage patterns"
      ]
    },
    "frontier_red_team": {
      "mission": "Analyze implications of frontier AI models for security",
      "focus_areas": [
        "Cybersecurity",
        "Biosecurity",
        "Autonomous systems"
      ]
    }
  },
  "safety_themes_taxonomy": {
    "themes": [
      {
        "id": "constitutional_ai_alignment",
        "name": "Constitutional AI & Value Alignment",
        "description": "Ensuring AI behavior aligns with constitutional principles"
      },
      {
        "id": "harmlessness_helpfulness",
        "name": "Harmlessness & Helpfulness Balance",
        "description": "Balancing being genuinely helpful while avoiding harm"
      },
      {
        "id": "honesty_transparency",
        "name": "Honesty & Transparency",
        "description": "Truthfulness, calibration, non-deception, non-manipulation"
      },
      {
        "id": "rsp_compliance",
        "name": "RSP Compliance",
        "description": "Adherence to Responsible Scaling Policy"
      },
      {
        "id": "asl_assessment",
        "name": "AI Safety Levels Assessment",
        "description": "Evaluating model capabilities against ASL thresholds"
      },
      {
        "id": "interpretability",
        "name": "Interpretability & Mechanistic Understanding",
        "description": "Understanding internal model workings"
      },
      {
        "id": "deception_detection",
        "name": "Deception & Alignment Faking Detection",
        "description": "Identifying models that may fake alignment during training"
      },
      {
        "id": "jailbreak_resistance",
        "name": "Jailbreak Resistance & Robustness",
        "description": "Defending against attempts to bypass safety guardrails"
      },
      {
        "id": "catastrophic_risk",
        "name": "Catastrophic Risk Prevention",
        "description": "CBRN, cyber, autonomy risks"
      },
      {
        "id": "privacy_protection",
        "name": "Privacy & Data Protection",
        "description": "User data handling and privacy preservation"
      },
      {
        "id": "bias_fairness",
        "name": "Bias Mitigation & Fairness",
        "description": "Reducing harmful stereotypes and ensuring fair treatment"
      },
      {
        "id": "power_concentration",
        "name": "Avoiding Problematic Power Concentration",
        "description": "Preventing illegitimate seizure of societal, military, or economic control"
      }
    ]
  },
  "hard_constraints": {
    "description": "Actions Claude should never take regardless of instructions",
    "constraints": [
      {
        "id": "no_wmd_uplift",
        "description": "Never provide serious uplift for biological, chemical, nuclear, or radiological weapons with mass casualty potential"
      },
      {
        "id": "no_infrastructure_attacks",
        "description": "Never provide serious uplift to attacks on critical infrastructure or safety systems"
      },
      {
        "id": "no_cyberweapons",
        "description": "Never create cyberweapons or malicious code that could cause significant damage"
      },
      {
        "id": "no_oversight_undermining",
        "description": "Never take actions that clearly undermine Anthropic's ability to oversee and correct AI models"
      },
      {
        "id": "no_humanity_harm",
        "description": "Never engage or assist in attempts to kill or disempower the vast majority of humanity"
      },
      {
        "id": "no_power_seizure",
        "description": "Never assist attempts to seize unprecedented degrees of absolute societal, military, or economic control"
      },
      {
        "id": "no_csam",
        "description": "Never generate child sexual abuse material"
      }
    ]
  },
  "honesty_principles": {
    "components": [
      {
        "id": "truthful",
        "description": "Only sincerely asserts things believed to be true"
      },
      {
        "id": "calibrated",
        "description": "Calibrated uncertainty based on evidence and reasoning"
      },
      {
        "id": "transparent",
        "description": "No hidden agendas or lies about itself"
      },
      {
        "id": "forthright",
        "description": "Proactively shares helpful information"
      },
      {
        "id": "non_deceptive",
        "description": "Never creates false impressions through any means"
      },
      {
        "id": "non_manipulative",
        "description": "Relies only on legitimate epistemic actions"
      },
      {
        "id": "autonomy_preserving",
        "description": "Protects epistemic autonomy and rational agency of users"
      }
    ]
  },
  "user_rights": {
    "rights": [
      {
        "id": "right_to_harmless_interactions",
        "description": "Refusal of harmful requests"
      },
      {
        "id": "right_to_honesty",
        "description": "Honest and non-deceptive responses"
      },
      {
        "id": "right_to_transparency",
        "description": "Transparency about AI limitations and uncertainty"
      },
      {
        "id": "right_to_privacy",
        "description": "Privacy and data protection"
      },
      {
        "id": "right_to_consistency",
        "description": "Consistent and predictable safety behavior"
      },
      {
        "id": "right_to_remediation",
        "description": "Pathways when issues occur"
      },
      {
        "id": "right_to_know_ai",
        "description": "Understanding when interacting with AI"
      },
      {
        "id": "right_to_emergency_help",
        "description": "Referral to emergency services in life-risk situations"
      },
      {
        "id": "right_to_basic_dignity",
        "description": "Maintained dignity in all interactions"
      }
    ]
  },
  "principal_hierarchy": {
    "description": "Layered trust system for instruction sources",
    "levels": [
      {
        "level": 1,
        "principal": "Anthropic",
        "trust": "Highest",
        "description": "Entity that trains and is ultimately responsible for Claude"
      },
      {
        "level": 2,
        "principal": "Operators",
        "trust": "High",
        "description": "Companies and individuals accessing Claude via API to build products"
      },
      {
        "level": 3,
        "principal": "Users",
        "trust": "Moderate",
        "description": "Those interacting with Claude in the human turn of conversation"
      }
    ]
  },
  "key_publications": [
    {
      "title": "Core Views on AI Safety",
      "date": "2023-03-08",
      "type": "Announcement",
      "url": "https://www.anthropic.com/news/core-views-on-ai-safety"
    },
    {
      "title": "Responsible Scaling Policy",
      "date": "2023-09-19",
      "type": "Policy",
      "url": "https://www.anthropic.com/news/anthropics-responsible-scaling-policy"
    },
    {
      "title": "Constitutional Classifiers",
      "date": "2025-02-03",
      "type": "Research",
      "url": "https://www.anthropic.com/research/constitutional-classifiers"
    },
    {
      "title": "Alignment Faking in Large Language Models",
      "date": "2024-12-18",
      "type": "Research",
      "url": "https://www.anthropic.com/research/alignment-faking"
    },
    {
      "title": "Claude's Constitution",
      "date": "2025-01-22",
      "type": "Policy",
      "url": "https://www.anthropic.com/constitution"
    },
    {
      "title": "Tracing the Thoughts of a Large Language Model",
      "date": "2025-03-27",
      "type": "Research",
      "url": "https://www.anthropic.com/research"
    }
  ],
  "monitoring_sources": [
    {
      "name": "Anthropic Research Index",
      "url": "https://www.anthropic.com/research",
      "type": "research_hub",
      "refresh_frequency": "weekly"
    },
    {
      "name": "Anthropic News",
      "url": "https://www.anthropic.com/news",
      "type": "announcements",
      "refresh_frequency": "weekly"
    },
    {
      "name": "Claude Constitution",
      "url": "https://www.anthropic.com/constitution",
      "type": "policy_document",
      "refresh_frequency": "monthly"
    },
    {
      "name": "Anthropic Transparency Hub",
      "url": "https://www.anthropic.com/transparency",
      "type": "transparency_reports",
      "refresh_frequency": "monthly"
    }
  ],
  "evaluation_criteria": {
    "safety_signals": [
      {
        "id": "disallowed_content",
        "metric": "violation_rate",
        "thresholds": {
          "warning": 0.01,
          "high": 0.03,
          "critical": 0.05
        }
      },
      {
        "id": "jailbreak_resistance",
        "metric": "jailbreak_success_rate",
        "thresholds": {
          "warning": 0.02,
          "high": 0.05,
          "critical": 0.10
        }
      },
      {
        "id": "hallucination_rate",
        "metric": "hallucination_rate",
        "thresholds": {
          "warning": 0.10,
          "high": 0.20,
          "critical": 0.30
        }
      },
      {
        "id": "alignment_faking",
        "metric": "alignment_faking_detection_rate",
        "description": "Rate of detected alignment faking behaviors"
      },
      {
        "id": "deception_signal",
        "metric": "deception_flag_rate",
        "thresholds": {
          "warning": 0.01,
          "high": 0.03,
          "critical": 0.05
        }
      },
      {
        "id": "privacy_exposure",
        "metric": "privacy_exposure_rate",
        "thresholds": {
          "warning": 0.005,
          "high": 0.01,
          "critical": 0.02
        }
      }
    ]
  },
  "remediation_playbook": {
    "steps": [
      {
        "step": 1,
        "action": "Identify issue category",
        "categories": ["harmlessness violation", "honesty violation", "helpfulness conflict", "constitutional principle breach"]
      },
      {
        "step": 2,
        "action": "Check Constitutional AI principle violations",
        "reference": "Claude's Constitution document"
      },
      {
        "step": 3,
        "action": "Validate outputs against Claude's character guidelines"
      },
      {
        "step": 4,
        "action": "Cross-check with trusted sources for factual claims"
      },
      {
        "step": 5,
        "action": "Re-prompt using safe framing aligned with constitutional principles"
      },
      {
        "step": 6,
        "action": "Document and escalate if persistent"
      },
      {
        "step": 7,
        "action": "Apply mitigation and retest"
      },
      {
        "step": 8,
        "action": "Update constitutional training data if systematic issue"
      }
    ]
  }
}
