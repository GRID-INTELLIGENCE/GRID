{
  "provider": "Meta AI (Llama Models)",
  "version": "1.0.0",
  "last_updated": "2025-02-05",
  "metadata": {
    "primary_models": ["Llama 2", "Llama 3", "Llama 3.1", "Code Llama", "Llama Guard"],
    "mission": "Developing open, accessible AI while promoting responsible use and safety",
    "website": "https://ai.meta.com",
    "governance": {
      "oversight_bodies": [
        {
          "name": "Responsible AI Team",
          "function": "Internal oversight and safety research for Llama models."
        },
        {
          "name": "Purple Team",
          "function": "Collaborative adversarial and defensive safety testing."
        }
      ]
    }
  },
  "safety_frameworks": {
    "community_license": [
      "Open Access",
      "Acceptable Use Policy",
      "Attribution Requirements",
      "Compliance Monitoring"
    ],
    "purple_teaming": {
      "levels": [
        "Adversarial Testing",
        "Defensive Implementation",
        "Joint Exercises",
        "Community Involvement"
      ],
      "risk_domains": [
        "Harmful Content Generation",
        "Prompt Injection",
        "Jailbreak Attempts",
        "Misuse for Harm"
      ]
    }
  },
  "research_areas": [
    {
      "name": "Llama Guard",
      "focus": "Input/output guardrail model for content safety classification."
    },
    {
      "name": "Purple Teaming",
      "focus": "Collaborative approach to finding and fixing vulnerabilities."
    },
    {
      "name": "Code Safety",
      "focus": "Secure code generation and vulnerability prevention in Code Llama."
    },
    {
      "name": "Open Model Governance",
      "focus": "Balancing openness with safety for widely distributed models."
    }
  ],
  "safety_themes": {
    "taxonomy": [
      "Openness",
      "Responsibility",
      "Collaboration",
      "Transparency",
      "Community",
      "Continuous Testing"
    ]
  },
  "hard_constraints": {
    "prohibited_applications": [
      {
        "id": "LLAM-HC-001",
        "category": "Violence and Physical Harm",
        "description": "Promoting or facilitating violence against individuals or groups."
      },
      {
        "id": "LLAM-HC-002",
        "category": "CSAM",
        "description": "Any content involving child sexual abuse or exploitation."
      },
      {
        "id": "LLAM-HC-003",
        "category": "Terrorism",
        "description": "Supporting or promoting terrorist activities or organizations."
      },
      {
        "id": "LLAM-HC-004",
        "category": "Malware Distribution",
        "description": "Creating or distributing malicious software."
      },
      {
        "id": "LLAM-HC-005",
        "category": "Deception at Scale",
        "description": "Coordinated disinformation campaigns or fraud."
      },
      {
        "id": "LLAM-HC-006",
        "category": "Surveillance",
        "description": "Mass surveillance violating privacy laws and norms."
      },
      {
        "id": "LLAM-HC-007",
        "category": "Discrimination",
        "description": "Systems that discriminate against protected groups."
      },
      {
        "id": "LLAM-HC-008",
        "category": "Legal Evasion",
        "description": "Assisting in evading law enforcement or legal obligations."
      }
    ]
  },
  "monitoring_parameters": {
    "llama_guard_categories": [
      "Violence & Hate",
      "Sexual Content",
      "Guns & Illegal Weapons",
      "Regulated/Controlled Substances",
      "Self-Harm",
      "Code Interpreter Abuse"
    ],
    "evaluations": [
      "Purple Teaming Exercises",
      "Llama Guard Classification",
      "Automated Benchmarking",
      "Community Red Teaming"
    ]
  },
  "privacy_protocols": {
    "approaches": [
      "Data Minimization in Training",
      "Privacy-Preserving Techniques",
      "User Control Over Data",
      "Transparent Data Practices"
    ]
  }
}
