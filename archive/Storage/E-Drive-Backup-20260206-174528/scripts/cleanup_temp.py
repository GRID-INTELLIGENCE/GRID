#!/usr/bin/env python3
"""
Automated cleanup script for workspace.
Removes temporary files, build artifacts, and old logs.
Outputs JSON summary for Cascade integration.
"""

import os
import json
import shutil
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List
import argparse

# Default cleanup targets
CLEANUP_TARGETS = {
    "__pycache__": {
        "type": "directory",
        "pattern": "__pycache__",
        "description": "Python cache directories"
    },
    "pyc_files": {
        "type": "file",
        "pattern": "*.pyc",
        "description": "Python bytecode files"
    },
    "basedpyright": {
        "type": "directory",
        "pattern": ".basedpyright",
        "description": "BasedPyright baseline cache (regenerated by tooling)"
    },
    "eufle_rag_cache": {
        "type": "directory",
        "pattern": [".eufle_rag", ".eufle_rag_demo", ".eufle_rag_test"],
        "description": "EUFLE RAG Chroma caches (optional; indices can be rebuilt)"
    },
    "build_artifacts": {
        "type": "directory",
        "pattern": ["dist", "build", ".next", "out"],
        "description": "Build output directories"
    },
    "analysis_outputs": {
        "type": "directory",
        "pattern": "analysis_outputs",
        "days_old": 30,
        "description": "Old analysis outputs (30+ days)"
    },
    "logs": {
        "type": "file",
        "pattern": "*.log",
        "days_old": 7,
        "description": "Old log files (7+ days)"
    },
    "temp_files": {
        "type": "file",
        "pattern": ["*.tmp", "*.temp", "*.cache"],
        "description": "Temporary files"
    }
}

# Archive directory
ARCHIVE_DIR = Path("e:/") / "archive"

# Skip system-managed dirs (access denied on Windows)
EXCLUDED_NAMES = {"$RECYCLE.BIN", "System Volume Information"}


def should_delete(path: Path, days_old: int = None) -> bool:
    """Check if path should be deleted based on age."""
    if days_old is None:
        return True
    
    if not path.exists():
        return False
    
    if path.is_file():
        mtime = datetime.fromtimestamp(path.stat().st_mtime)
    else:
        mtime = datetime.fromtimestamp(path.stat().st_mtime)
    
    age = datetime.now() - mtime
    return age.days >= days_old


def is_excluded(path: Path, workspace_root: Path) -> bool:
    """Skip system-managed dirs and workspace archive."""
    for part in path.parts:
        if part in EXCLUDED_NAMES:
            return True
    try:
        archive_dir = workspace_root / "archive"
        if archive_dir.exists() and archive_dir in path.parents:
            return True
    except Exception:
        pass
    return False


def cleanup_pattern(base_path: Path, pattern: Dict, workspace_root: Path, dry_run: bool = False) -> Dict:
    """Clean up files/directories matching a pattern."""
    results = {
        "deleted": [],
        "archived": [],
        "errors": [],
        "bytes_freed": 0
    }
    
    pattern_str = pattern.get("pattern", [])
    if not isinstance(pattern_str, list):
        pattern_str = [pattern_str]
    
    pattern_type = pattern.get("type", "file")
    days_old = pattern.get("days_old")
    
    for pat in pattern_str:
        if pattern_type == "directory":
            for item in base_path.rglob(pat):
                if is_excluded(item, workspace_root):
                    continue
                if not item.is_dir() or not should_delete(item, days_old):
                    continue
                try:
                    size = sum(f.stat().st_size for f in item.rglob('*') if f.is_file())
                    if dry_run:
                        results["deleted"].append(str(item))
                        results["bytes_freed"] += size
                        continue
                    if pattern.get("archive", False):
                        archive_path = ARCHIVE_DIR / item.relative_to(workspace_root)
                        archive_path.parent.mkdir(parents=True, exist_ok=True)
                        shutil.move(str(item), str(archive_path))
                        results["archived"].append(str(item))
                    else:
                        shutil.rmtree(item)
                        results["deleted"].append(str(item))
                    results["bytes_freed"] += size
                except Exception as e:
                    results["errors"].append(f"{item}: {str(e)}")
        elif pattern_type == "file":
            for item in base_path.rglob(pat):
                if is_excluded(item, workspace_root):
                    continue
                if not item.is_file() or not should_delete(item, days_old):
                    continue
                try:
                    size = item.stat().st_size
                    if not dry_run:
                        item.unlink()
                    results["deleted"].append(str(item))
                    results["bytes_freed"] += size
                except Exception as e:
                    results["errors"].append(f"{item}: {str(e)}")
    return results


def cleanup_workspace(workspace_root: Path, dry_run: bool = False) -> Dict:
    """Perform workspace cleanup."""
    print(f"Workspace cleanup: {workspace_root}")
    print(f"Mode: {'DRY RUN' if dry_run else 'CLEANUP'}")
    print("=" * 50)
    
    all_results = {
        "timestamp": datetime.now().isoformat(),
        "workspace_root": str(workspace_root),
        "dry_run": dry_run,
        "targets": {}
    }
    
    total_bytes_freed = 0
    
    for target_name, pattern_config in CLEANUP_TARGETS.items():
        print(f"\n{pattern_config['description']}...")
        results = cleanup_pattern(workspace_root, pattern_config, workspace_root, dry_run=dry_run)

        deleted_count = len(results["deleted"])
        archived_count = len(results["archived"])
        error_count = len(results["errors"])

        if dry_run and deleted_count > 0:
            print(f"  Would delete {deleted_count} items")
            print(f"  Would free {results['bytes_freed']:,} bytes")
        else:
            if deleted_count > 0:
                print(f"  Deleted {deleted_count} items")
            if archived_count > 0:
                print(f"  Archived {archived_count} items")
            if error_count > 0:
                print(f"  Errors: {error_count}")
            if results["bytes_freed"] > 0:
                size_mb = results["bytes_freed"] / (1024 * 1024)
                print(f"  Freed {size_mb:.2f} MB")
        if results["bytes_freed"] > 0:
            total_bytes_freed += results["bytes_freed"]

        all_results["targets"][target_name] = results
    
    all_results["total_bytes_freed"] = total_bytes_freed
    all_results["total_bytes_freed_mb"] = total_bytes_freed / (1024 * 1024)
    
    print("\n" + "=" * 50)
    print("Cleanup Summary")
    print("=" * 50)
    
    if dry_run:
        print("DRY RUN: No files were actually deleted")
        print(f"Would free (total): {all_results['total_bytes_freed_mb']:.2f} MB")
    else:
        print(f"Total space freed: {all_results['total_bytes_freed_mb']:.2f} MB")
    
    return all_results


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(description='Cleanup workspace temporary files')
    parser.add_argument('--workspace-root', default='e:\\', help='Workspace root directory')
    parser.add_argument('--dry-run', action='store_true', help='Perform dry run without deleting')
    parser.add_argument('--output-json', action='store_true', help='Output JSON summary')
    parser.add_argument('--output-file', help='JSON output file path')
    
    args = parser.parse_args()
    
    workspace_root = Path(args.workspace_root).resolve()
    
    if not workspace_root.exists():
        print(f"Error: Workspace root does not exist: {workspace_root}")
        return 1
    
    results = cleanup_workspace(workspace_root, dry_run=args.dry_run)
    
    # Output JSON if requested (for Cascade)
    if args.output_json or args.output_file:
        output_file = Path(args.output_file) if args.output_file else workspace_root / "cleanup_summary.json"
        with open(output_file, 'w') as f:
            json.dump(results, f, indent=2)
        print(f"\nJSON summary saved to: {output_file}")
    
    return 0


if __name__ == '__main__':
    import sys
    sys.exit(main())