# WSL + llama-cli Setup - Complete Runbook

**Date:** January 18, 2026  
**Status:** Diagnostics created, ready for execution

---

## Quick Start - Immediate Actions

### For Windows Users (Choice of 2 paths):

#### ğŸš€ Path A: Ollama (Works Now - NO Setup Needed)
```powershell
$env:EUFLE_DEFAULT_PROVIDER = "ollama"
ollama serve                          # In separate terminal
python eufle.py --ask
```

#### ğŸ”§ Path B: WSL + llama-cli (Needs setup)

**1. Check if setup is possible:**
```cmd
quick_diagnostic.bat
```
or
```powershell
powershell -ExecutionPolicy Bypass -File quick_diagnostic.ps1
```

**2. If TEST 4 shows "âœ“ YES":**
```powershell
wsl bash /mnt/e/EUFLE/scripts/setup_llamacpp.sh
```

**3. If TEST 4 shows "âœ— NO":**
```cmd
FIX_WSL_MOUNT.bat
```
Then follow the instructions to fix the mount.

---

## Detailed Setup Steps

### Step 1: Check Current Status

Run the quick diagnostic:
```powershell
# PowerShell version (recommended)
powershell -ExecutionPolicy Bypass -File quick_diagnostic.ps1

# Or batch version
quick_diagnostic.bat
```

This will check:
- âœ“ WSL installation
- âœ“ Available distros
- âœ“ Windows E: drive
- âœ“ WSL /mnt/e mount
- âœ“ Setup script existence

### Step 2: If /mnt/e/EUFLE is NOT accessible

**Run the fix:**
```cmd
FIX_WSL_MOUNT.bat
```

**Or manually:**

1. Open WSL terminal:
```powershell
wsl
```

2. Edit the config:
```bash
sudo nano /etc/wsl.conf
```

3. Add/ensure these lines exist:
```ini
[automount]
enabled = true
options = "metadata"
```

4. Save: `Ctrl+O` â†’ `Enter` â†’ `Ctrl+X`

5. Exit and restart WSL:
```bash
exit
```
```powershell
wsl --shutdown
# Then open WSL again
wsl
```

6. Verify:
```bash
ls -la /mnt/e/EUFLE/
```

Should show: `scripts/`, `eufle/`, `README.md`, etc.

### Step 3: Run the Setup

Once `/mnt/e/EUFLE` is accessible:

```bash
cd /mnt/e/EUFLE
bash scripts/setup_llamacpp.sh
```

Or from PowerShell:
```powershell
wsl bash /mnt/e/EUFLE/scripts/setup_llamacpp.sh
```

### Step 4: Configure Environment

After setup completes:
```powershell
# Get the EUFLE root directory dynamically
$eufleRoot = (Get-Item $PSScriptRoot -ErrorAction SilentlyContinue).Parent.FullName
if (-not (Test-Path "$eufleRoot\EUFLE")) {
    $eufleRoot = $PSScriptRoot
}

$env:EUFLE_ROOT = $eufleRoot
$env:EUFLE_DEFAULT_PROVIDER = "llamacpp"
$env:EUFLE_GGUF_MODEL = "$eufleRoot\EUFLE\hf-models\ministral-14b\consolidated.safetensors"

# Optional - make permanent
[Environment]::SetEnvironmentVariable("EUFLE_ROOT", $eufleRoot, "User")
[Environment]::SetEnvironmentVariable("EUFLE_DEFAULT_PROVIDER", "llamacpp", "User")
[Environment]::SetEnvironmentVariable("EUFLE_GGUF_MODEL", "$eufleRoot\EUFLE\hf-models\ministral-14b\consolidated.safetensors", "User")
```

### Step 5: Test

```powershell
python eufle.py --ask
```

---

## Available Diagnostic & Setup Scripts

### PowerShell Scripts

| Script | Purpose | Run With |
|--------|---------|----------|
| `quick_diagnostic.ps1` | Quick WSL status check | `powershell -ExecutionPolicy Bypass -File quick_diagnostic.ps1` |
| `setup_wsl_complete.ps1` | Full diagnostics + instructions | `powershell -ExecutionPolicy Bypass -File setup_wsl_complete.ps1` |
| `setup_ollama_eufle.ps1` | Ollama configuration | `.\setup_ollama_eufle.ps1 -Permanent` |

### Batch Scripts

| Script | Purpose | Run With |
|--------|---------|----------|
| `quick_diagnostic.bat` | Quick WSL status check | `quick_diagnostic.bat` |
| `FIX_WSL_MOUNT.bat` | WSL mount fix instructions | `FIX_WSL_MOUNT.bat` |

### Python Scripts

| Script | Purpose | Run With |
|--------|---------|----------|
| `verify_eufle_setup.py` | Full setup verification | `python verify_eufle_setup.py` |
| `EUFLE/scripts/setup_llamacpp.sh` | Build llama-cli (WSL) | `wsl bash /mnt/e/EUFLE/scripts/setup_llamacpp.sh` |

---

## Three Configuration Options Summary

### Option 1: Ollama âœ… (Ready Now)
- **Status:** Can use immediately
- **Setup time:** 2 minutes
- **Command:** `python eufle.py --ask`
- **Complexity:** Easiest

### Option 2: WSL + llama-cli âš™ï¸ (Needs Setup)
- **Status:** Infrastructure ready, mount may need fix
- **Setup time:** 10-15 minutes (including fix if needed)
- **Command:** `wsl bash /mnt/e/EUFLE/scripts/setup_llamacpp.sh`
- **Complexity:** Medium

### Option 3: Local HF Model âœ… (Already Works)
- **Status:** Automatic, no setup
- **Setup time:** 0 minutes
- **Command:** `python eufle.py --ask` (automatic fallback)
- **Complexity:** None

---

## Troubleshooting

### "WSL not installed"
**Solution:** `wsl --install` then restart Windows

### "/mnt/e/EUFLE not accessible"
**Solution:** Run `FIX_WSL_MOUNT.bat` or follow manual fix in Step 2 above

### "setup_llamacpp.sh not found"
**Solution:** Check path: `ls /mnt/e/EUFLE/scripts/` in WSL

### "Script runs but takes very long"
**Solution:** Normal - building GGUF binary takes 5-10 minutes
- Keep terminal open and wait
- Check logs: `tail -f build.log` (if available)

### "Build fails with errors"
**Solution:** 
1. Check WSL has 4GB+ RAM: `wsl --list --verbose`
2. Update WSL: `wsl --update`
3. Run `wsl --shutdown` and try again

---

## File Locations Reference

```
$env:EUFLE_ROOT                  â† Root directory (auto-detected)
â”œâ”€â”€ quick_diagnostic.ps1         â† Quick diagnostic (PowerShell)
â”œâ”€â”€ quick_diagnostic.bat         â† Quick diagnostic (Batch)
â”œâ”€â”€ setup_wsl_complete.ps1       â† Full setup script
â”œâ”€â”€ setup_ollama_eufle.ps1       â† Ollama setup
â”œâ”€â”€ setup_ollama_eufle.bat       â† Ollama setup (CMD)
â”œâ”€â”€ FIX_WSL_MOUNT.bat            â† Mount fix instructions
â”œâ”€â”€ verify_eufle_setup.py        â† Verification tool
â”œâ”€â”€ EUFLE_SETUP_GUIDE.md         â† Full setup guide
â”œâ”€â”€ WSL_MOUNT_FIX.md             â† Mount fix details
â”œâ”€â”€ QUICK_REFERENCE.md           â† Quick reference card
â””â”€â”€ EUFLE\
    â”œâ”€â”€ eufle.py                 â† Main CLI
    â”œâ”€â”€ scripts\
    â”‚   â”œâ”€â”€ setup_llamacpp.sh    â† Build llama-cli
    â”‚   â”œâ”€â”€ wsl_bridge.py        â† WSL integration
    â”‚   â”œâ”€â”€ wsl_diagnostic.sh    â† WSL diagnostics
    â”‚   â””â”€â”€ ...
    â”œâ”€â”€ hf-models\
    â”‚   â””â”€â”€ ministral-14b\
    â”‚       â”œâ”€â”€ tokenizer.json   âœ“ Valid
    â”‚       â”œâ”€â”€ consolidated.safetensors
    â”‚       â””â”€â”€ config.json
    â””â”€â”€ studio\
        â””â”€â”€ unified_api.py       â† Provider selection
```

---

## Environment Variables

### For Ollama:
```powershell
$env:EUFLE_DEFAULT_PROVIDER = "ollama"
```

### For WSL + llama-cli:
```powershell
$eufleRoot = $env:EUFLE_ROOT  # Set via step 4 above
$env:EUFLE_DEFAULT_PROVIDER = "llamacpp"
$env:EUFLE_GGUF_MODEL = "$eufleRoot\EUFLE\hf-models\ministral-14b\consolidated.safetensors"
```

### Optional - HF Model:
```powershell
$env:EUFLE_DEFAULT_PROVIDER = "auto"  # Auto-fallback
```

---

## Next Actions (In Order)

1. âœ… **Run diagnostic:**
   ```
   quick_diagnostic.bat
   ```

2. ğŸ“‹ **Check the TEST 4 result**

3. âœ… **If TEST 4 = YES:**
   ```
   wsl bash /mnt/e/EUFLE/scripts/setup_llamacpp.sh
   ```

4. âš™ï¸ **If TEST 4 = NO:**
   ```
   FIX_WSL_MOUNT.bat
   â† Follow instructions
   â† Try again
   ```

5. ğŸš€ **After setup completes:**
   ```powershell
   $env:EUFLE_DEFAULT_PROVIDER = "llamacpp"
   python eufle.py --ask
   ```

---

## Support & Documentation

- Full setup guide: `EUFLE_SETUP_GUIDE.md`
- Mount fix guide: `WSL_MOUNT_FIX.md`
- Quick reference: `QUICK_REFERENCE.md`
- EUFLE docs: `$env:EUFLE_ROOT\EUFLE\README.md`
- WSL docs: `$env:EUFLE_ROOT\EUFLE\docs\wsl_setup.md`

---

**Status:** All setup infrastructure is ready. Execute diagnostics to proceed.
